{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoqkeras-tfdata-tfc-main",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Cu-SB4Yx0Tzt",
        "outputId": "d4dbb19c-1fa4-407d-9745-138571697d5b"
      },
      "source": [
        "%pip install qkeras tensorflow-io tensorflow-cloud"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: qkeras in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Collecting tensorflow-cloud\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/bc/da205a15aaf22c1fda1f58552990d17d532a8573af6830e3663730ed485b/tensorflow_cloud-0.1.16-py3-none-any.whl (92kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-model-optimization>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (0.6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.4.1)\n",
            "Requirement already satisfied: keras-tuner>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.0.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (57.0.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.7/dist-packages (from qkeras) (4.61.2)\n",
            "Requirement already satisfied: scikit-learn>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from qkeras) (0.24.2)\n",
            "Requirement already satisfied: pyparser in /usr/local/lib/python3.7/dist-packages (from qkeras) (1.0)\n",
            "Requirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (0.19.0)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (1.12.8)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (1.32.1)\n",
            "Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (2.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (0.12.0)\n",
            "Collecting docker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/5a/f988909dfed18c1ac42ad8d9e611e6c5657e270aa6eb68559985dbb69c13/docker-5.0.0-py2.py3-none-any.whl (146kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (1.18.1)\n",
            "Collecting tensorflow-transform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e2/00e08e6897376015de67e5c284f525adc48755f8c108fc49f008172aef51/tensorflow_transform-1.1.0-py3-none-any.whl (401kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tensorflow-cloud) (4.0.1)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (1.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (0.1.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (20.9)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (5.5.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.1->qkeras) (1.0.3)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->qkeras) (4.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->qkeras) (2.2.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.23.1->qkeras) (1.0.1)\n",
            "Requirement already satisfied: parse==1.6.5 in /usr/local/lib/python3.7/dist-packages (from pyparser->qkeras) (1.6.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (2.5.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.34.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow-io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensorflow-cloud) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensorflow-cloud) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensorflow-cloud) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensorflow-cloud) (0.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth->tensorflow-cloud) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth->tensorflow-cloud) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth->tensorflow-cloud) (4.2.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud) (1.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud) (0.4.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.3.0->tensorflow-cloud) (1.0.1)\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/5f/3c211d168b2e9f9342cfb53bcfc26aab0eac63b998015e7af7bcae66119d/websocket_client-1.1.0-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->tensorflow-cloud) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage->tensorflow-cloud) (0.4.1)\n",
            "Collecting apache-beam[gcp]<3,>=2.29\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/f0/83e04f7a693695f4ce3765fce1e573abbbf32153a309829651de056f8924/apache_beam-2.31.0-cp37-cp37m-manylinux2010_x86_64.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 10.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata<1.2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform->tensorflow-cloud) (1.1.0)\n",
            "Collecting tfx-bsl<1.2.0,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/bb/c2164101365997e8a7ea25275ab0ae12def11ef91e5127a0ed36a6939c51/tfx_bsl-1.1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (19.0MB)\n",
            "\u001b[K     |████████████████████████████████| 19.0MB 400kB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-transform->tensorflow-cloud) (1.3.0)\n",
            "Collecting pyarrow<3,>=1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/8d/c002e27767595f22aa09ed0d364327922f673d12b36526c967a2bf6b2ed7/pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 454kB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensorflow-cloud) (21.2.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensorflow-cloud) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensorflow-cloud) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensorflow-cloud) (0.3.4)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tensorflow-cloud) (5.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.1->qkeras) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner>=1.0.1->qkeras) (2.4.7)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (5.0.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.1->qkeras) (2.6.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow-io) (1.5.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensorflow-cloud) (2018.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensorflow-cloud) (1.53.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth->tensorflow-cloud) (0.4.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.3.0->tensorflow-cloud) (4.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->tensorflow-cloud) (1.3.0)\n",
            "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting fastavro<2,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/92/10ee74edb0a39f4a7af1cf271b3ac725c54f5c243c26fa5059cd794d15d7/fastavro-1.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 29.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (2.8.1)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (3.11.4)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (4.1.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (1.7)\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl\n",
            "Collecting google-cloud-vision<2,>=0.38.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0d/7f/e10d602c2dc3f749f1b78377a3357790f1da71b28e7da9e5bc20b3a9bd40/google_cloud_vision-1.0.0-py2.py3-none-any.whl (435kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 51.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/17/536768bada8f93f124826b36dbdcdf08edd0e5ef0ca76b4c911f9f28596a/google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 43.6MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/b3/dd83eca4cd1019d592e82595ea45d53f11e39db4ee99daa66ceb8a1b2d89/google_cloud_pubsub-1.7.0-py2.py3-none-any.whl (144kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (1.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<3,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.7/dist-packages (from apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (1.21.0)\n",
            "Collecting grpcio-gcp<1,>=0.2.2; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/83/1f1095815be0de19102df41e250ebbd7dae97d7d14e22c18da07ed5ed9d4/grpcio_gcp-0.2.2-py2.py3-none-any.whl\n",
            "Collecting google-cloud-language<2,>=1.3.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b8/965a97ba60287910d342623da1da615254bded3e0965728cf7fc6339b7c8/google_cloud_language-1.3.0-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-profiler<4,>=3.0.4; extra == \"gcp\"\n",
            "  Downloading https://files.pythonhosted.org/packages/02/6f/719457d1017aceacfabda6e4de04cd72b453a7ed45f43334fa615e6efa01/google-cloud-profiler-3.0.5.tar.gz\n",
            "Collecting google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/da/aefc4cf4c168b5d875344cd9dddc77e3a2d11986b630251af5ce47dd2843/google-apitools-0.5.31.tar.gz (173kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 49.2MB/s \n",
            "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/29/8d06211102c87768dc34943d9c92abd8b67491ffedd6d09b56305b1ab255/google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 51.8MB/s \n",
            "\u001b[?25hCollecting google-cloud-spanner<2,>=1.13.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/16/c9262ca40f3a278f38df9d21dece1ae01ee24f8ed29937bd1f066f908f9f/google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 55.1MB/s \n",
            "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/c3/5b73c15f59207b20df288573c2ea203c7b126df8330add380d8b50bc0d5c/google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 55.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/27/a6/a534deae4086c0fef9a77537a4779bb5a9dd8841f8d347c509c96b342b2e/tensorflow_serving_api-2.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tfx-bsl<1.2.0,>=1.1.0->tensorflow-transform->tensorflow-cloud) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->tensorflow-cloud) (3.4.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner>=1.0.1->qkeras) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->keras-tuner>=1.0.1->qkeras) (0.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->tensorflow-cloud) (3.1.1)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.29->tensorflow-transform->tensorflow-cloud) (0.6.2)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/31/91/6630ebd169ca170634ca8a10dfcc5f5c11b0621672d4c2c9e40381c6d81a/fasteners-0.16.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: avro-python3, google-cloud-profiler, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp37-none-any.whl size=43516 sha256=b9863f459ac6bcd758966ea42f2373977cd68ca7c6e6e7ceefdf7888e953d621\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for google-cloud-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-cloud-profiler: filename=google_cloud_profiler-3.0.5-cp37-cp37m-linux_x86_64.whl size=702372 sha256=319461eed2d7311854707cd32bd29d8f1460914734b2b586d34186a99803c988\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/0c/3b/b9d175a70c463949099954744338a8a1883de5f08ec8d6e465\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-cp37-none-any.whl size=131043 sha256=17a0998206ce502a7131f41e65c528f50de283b2d58ce01bceaaf9559fa56988\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/43/31/09a9dad88d3aec6fed2d63bd35dfc532fca372e2edec5af5bf\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp37-none-any.whl size=18516 sha256=398f4a9e9a22a6f188bc8bf4c6e36543e5e7b8bac36bee10776ab907865e068a\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built avro-python3 google-cloud-profiler google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: google-cloud-bigtable 1.7.0 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-cloud-spanner 1.19.1 has requirement google-cloud-core<2.0dev,>=1.4.1, but you'll have google-cloud-core 1.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.31.0 has requirement dill<0.3.2,>=0.3.1.1, but you'll have dill 0.3.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.31.0 has requirement future<1.0.0,>=0.18.2, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: apache-beam 2.31.0 has requirement requests<3.0.0,>=2.24.0, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: websocket-client, docker, pyarrow, avro-python3, fastavro, hdfs, google-cloud-vision, grpc-google-iam-v1, google-cloud-bigtable, google-cloud-pubsub, grpcio-gcp, google-cloud-language, google-cloud-profiler, fasteners, google-apitools, google-cloud-videointelligence, google-cloud-spanner, google-cloud-dlp, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-transform, tensorflow-cloud\n",
            "  Found existing installation: pyarrow 3.0.0\n",
            "    Uninstalling pyarrow-3.0.0:\n",
            "      Successfully uninstalled pyarrow-3.0.0\n",
            "  Found existing installation: google-cloud-language 1.2.0\n",
            "    Uninstalling google-cloud-language-1.2.0:\n",
            "      Successfully uninstalled google-cloud-language-1.2.0\n",
            "Successfully installed apache-beam-2.31.0 avro-python3-1.9.2.1 docker-5.0.0 fastavro-1.4.2 fasteners-0.16.3 google-apitools-0.5.31 google-cloud-bigtable-1.7.0 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-profiler-3.0.5 google-cloud-pubsub-1.7.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpc-google-iam-v1-0.12.3 grpcio-gcp-0.2.2 hdfs-2.6.0 pyarrow-2.0.0 tensorflow-cloud-0.1.16 tensorflow-serving-api-2.5.1 tensorflow-transform-1.1.0 tfx-bsl-1.1.0 websocket-client-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ixWlfvvIcZ0"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc72b81a"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.models import Model,model_from_json\n",
        "from tensorflow.keras.layers import Input, InputLayer, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
        "from tensorflow.keras import backend as K\n",
        "from qkeras import QDense, QActivation\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from datetime import datetime\n",
        "from tensorboard import program\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import pathlib\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "\n",
        "#from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c66ed1a"
      },
      "source": [
        "## for Vizier\n",
        "import tensorflow_cloud as tfc\n",
        "import kerastuner"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMwhDBjUIj3B"
      },
      "source": [
        "# Define\n",
        "Several utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trcRDqMKHzdU"
      },
      "source": [
        "def preprocess_anomaly_data(pT_scaler, anomaly_data):\n",
        "    anomaly_data[:,9:19,0] = np.where(anomaly_data[:,9:19,1]>4,0,anomaly_data[:,9:19,0])\n",
        "    anomaly_data[:,9:19,0] = np.where(anomaly_data[:,9:19,1]<-4,0,anomaly_data[:,9:19,0])\n",
        "    anomaly_data[:,9:19,1] = np.where(anomaly_data[:,9:19,1]>4,0,anomaly_data[:,9:19,1])\n",
        "    anomaly_data[:,9:19,1] = np.where(anomaly_data[:,9:19,1]<-4,0,anomaly_data[:,9:19,1])\n",
        "    anomaly_data[:,9:19,2] = np.where(anomaly_data[:,9:19,1]>4,0,anomaly_data[:,9:19,2])\n",
        "    anomaly_data[:,9:19,2] = np.where(anomaly_data[:,9:19,1]<-4,0,anomaly_data[:,9:19,2])\n",
        "    \n",
        "    data_noMET = anomaly_data[:,1:,:]\n",
        "    MET = anomaly_data[:,0,[0,2]]\n",
        "\n",
        "    pT = data_noMET[:,:,0]\n",
        "    eta = data_noMET[:,:,1]\n",
        "    phi = data_noMET[:,:,2]\n",
        "\n",
        "    pT = np.concatenate((MET[:,0:1],pT), axis=1) # add MET pt for scaling\n",
        "    mask_pT = pT!=0\n",
        "\n",
        "    pT_scaled = np.copy(pT)\n",
        "    pT_scaled = pT_scaler.transform(pT_scaled)\n",
        "    pT_scaled = pT_scaled*mask_pT\n",
        "\n",
        "    phi = np.concatenate((MET[:,1:2], phi), axis=1)\n",
        "\n",
        "    test_scaled = np.concatenate((pT_scaled[:,0:1], pT_scaled[:,1:], eta, phi), axis=1)\n",
        "    test_notscaled = np.concatenate((MET[:,0:1], data_noMET[:,:,0], eta, phi), axis=1)\n",
        "    \n",
        "    return test_scaled, test_notscaled\n",
        "\n",
        "\n",
        "def custom_loss_negative(true, prediction):\n",
        "    \n",
        "    #mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "    # 0-1 = met(pt,phi) , 2-14 = egamma, 14-26 = muon, 26-56 = jet; (pt,eta,phi) order\n",
        "    #MASK PT\n",
        "    mask_met = tf.math.not_equal(true[:,0:1],0)\n",
        "    mask_met = tf.cast(mask_met, tf.float32)\n",
        "    mask_eg = tf.math.not_equal(true[:,1:5],0)\n",
        "    mask_eg = tf.cast(mask_eg, tf.float32)\n",
        "    mask_muon = tf.math.not_equal(true[:,5:9],0)\n",
        "    mask_muon = tf.cast(mask_muon, tf.float32)\n",
        "    mask_jet = tf.math.not_equal(true[:,9:19],0)\n",
        "    mask_jet = tf.cast(mask_jet, tf.float32)\n",
        "\n",
        "    # PT\n",
        "    met_pt_pred = tf.math.multiply(prediction[:,0:1],mask_met) #MET\n",
        "    jets_pt_pred = tf.math.multiply(prediction[:,9:19],mask_jet) #Jets\n",
        "    muons_pt_pred = tf.math.multiply(prediction[:,5:9],mask_muon) #Muons\n",
        "    eg_pt_pred = tf.math.multiply(prediction[:,1:5],mask_eg) #EGammas\n",
        "    \n",
        "    # ETA\n",
        "    jets_eta_pred = tf.math.multiply(4.0*(tf.math.tanh(prediction[:,27:37])),mask_jet) #Jets\n",
        "    muons_eta_pred = tf.math.multiply(2.1*(tf.math.tanh(prediction[:,23:27])),mask_muon) #Muons\n",
        "    eg_eta_pred = tf.math.multiply(3.0*(tf.math.tanh(prediction[:,19:23])),mask_eg) #EGammas\n",
        "    \n",
        "    # PHI\n",
        "    met_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,37:38]),mask_met) #MET\n",
        "    jets_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,46:56]),mask_jet) #Jets\n",
        "    muon_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,42:46]),mask_muon) #Muons\n",
        "    eg_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,38:42]),mask_eg) #EGammas\n",
        "    \n",
        "    y_pred = tf.concat([met_pt_pred, eg_pt_pred, muons_pt_pred, jets_pt_pred, eg_eta_pred, muons_eta_pred, jets_eta_pred,\\\n",
        "                       met_phi_pred, eg_phi_pred, muon_phi_pred, jets_phi_pred], axis=-1)\n",
        "    loss = tf.reduce_mean(tf.math.square(true - y_pred),axis=-1)\n",
        "    return -loss\n",
        "\n",
        "def custom_loss_training(true, prediction):\n",
        "    \n",
        "    #mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "    # 0-1 = met(pt,phi) , 2-14 = egamma, 14-26 = muon, 26-56 = jet; (pt,eta,phi) order\n",
        "    #MASK PT\n",
        "    mask_met = tf.math.not_equal(true[:,0:1],0)\n",
        "    mask_met = tf.cast(mask_met, tf.float32)\n",
        "    mask_eg = tf.math.not_equal(true[:,1:5],0)\n",
        "    mask_eg = tf.cast(mask_eg, tf.float32)\n",
        "    mask_muon = tf.math.not_equal(true[:,5:9],0)\n",
        "    mask_muon = tf.cast(mask_muon, tf.float32)\n",
        "    mask_jet = tf.math.not_equal(true[:,9:19],0)\n",
        "    mask_jet = tf.cast(mask_jet, tf.float32)\n",
        "\n",
        "    # PT\n",
        "    met_pt_pred = tf.math.multiply(prediction[:,0:1],mask_met) #MET\n",
        "    jets_pt_pred = tf.math.multiply(prediction[:,9:19],mask_jet) #Jets\n",
        "    muons_pt_pred = tf.math.multiply(prediction[:,5:9],mask_muon) #Muons\n",
        "    eg_pt_pred = tf.math.multiply(prediction[:,1:5],mask_eg) #EGammas\n",
        "    \n",
        "    # ETA\n",
        "    jets_eta_pred = tf.math.multiply(4.0*(tf.math.tanh(prediction[:,27:37])),mask_jet) #Jets\n",
        "    muons_eta_pred = tf.math.multiply(2.1*(tf.math.tanh(prediction[:,23:27])),mask_muon) #Muons\n",
        "    eg_eta_pred = tf.math.multiply(3.0*(tf.math.tanh(prediction[:,19:23])),mask_eg) #EGammas\n",
        "    \n",
        "    # PHI\n",
        "    met_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,37:38]),mask_met) #MET\n",
        "    jets_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,46:56]),mask_jet) #Jets\n",
        "    muon_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,42:46]),mask_muon) #Muons\n",
        "    eg_phi_pred = tf.math.multiply(math.pi*tf.math.tanh(prediction[:,38:42]),mask_eg) #EGammas\n",
        "    \n",
        "    y_pred = tf.concat([met_pt_pred, eg_pt_pred, muons_pt_pred, jets_pt_pred, eg_eta_pred, muons_eta_pred, jets_eta_pred,\\\n",
        "                       met_phi_pred, eg_phi_pred, muon_phi_pred, jets_phi_pred], axis=-1)\n",
        "    loss = tf.reduce_mean(tf.math.square(true - y_pred),axis=-1)\n",
        "    return loss\n",
        "\n",
        "def mse_loss(inputs, outputs):\n",
        "    return np.mean((inputs-outputs)*(inputs-outputs), axis=-1)\n",
        "\n",
        "def custom_loss_numpy(true, prediction):\n",
        "    #mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "    # 0-1 = met(pt,phi) , 2-14 = egamma, 14-26 = muon, 26-56 = jet; (pt,eta,phi) order\n",
        "    #MASK PT\n",
        "    mask_met = np.not_equal(true[:,0:1],0)\n",
        "    mask_eg = np.not_equal(true[:,1:5],0)\n",
        "    mask_muon = np.not_equal(true[:,5:9],0)\n",
        "    mask_jet = np.not_equal(true[:,9:19],0)\n",
        "\n",
        "    # PT\n",
        "    met_pt_pred = np.multiply(prediction[:,0:1],mask_met) #MET\n",
        "    jets_pt_pred = np.multiply(prediction[:,9:19],mask_jet) #Jets\n",
        "    muons_pt_pred = np.multiply(prediction[:,5:9],mask_muon) #Muons\n",
        "    eg_pt_pred = np.multiply(prediction[:,1:5],mask_eg) #EGammas\n",
        "    \n",
        "    # ETA\n",
        "    jets_eta_pred = np.multiply(4.0*(np.tanh(prediction[:,27:37])),mask_jet) #Jets\n",
        "    muons_eta_pred = np.multiply(2.1*(np.tanh(prediction[:,23:27])),mask_muon) #Muons\n",
        "    eg_eta_pred = np.multiply(3.0*(np.tanh(prediction[:,19:23])),mask_eg) #EGammas\n",
        "    \n",
        "    # PHI\n",
        "    met_phi_pred = np.multiply(math.pi*np.tanh(prediction[:,37:38]),mask_met) #MET\n",
        "    jets_phi_pred = np.multiply(math.pi*np.tanh(prediction[:,46:56]),mask_jet) #Jets\n",
        "    muon_phi_pred = np.multiply(math.pi*np.tanh(prediction[:,42:46]),mask_muon) #Muons\n",
        "    eg_phi_pred = np.multiply(math.pi*np.tanh(prediction[:,38:42]),mask_eg) #EGammas\n",
        "    \n",
        "    y_pred = np.concatenate([met_pt_pred, eg_pt_pred, muons_pt_pred, jets_pt_pred, eg_eta_pred, muons_eta_pred, jets_eta_pred,\\\n",
        "                       met_phi_pred, eg_phi_pred, muon_phi_pred, jets_phi_pred], axis=-1)\n",
        "    loss = mse_loss(true,y_pred)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def roc_objective(ae, X_test, bsm_data):\n",
        "    def roc_objective_val(y_true, y_pred):\n",
        "        # evaluate mse term\n",
        "        predicted_qcd = ae(X_test, training=False)\n",
        "        #mse_qcd = custom_loss_numpy(X_test, predicted_qcd.numpy()) ## THIS IS WHERE WE REQUIRE EAGER EXECUTION ##\n",
        "        mse_qcd = custom_loss_training(X_test, predicted_qcd) ## THIS IS WHERE WE REQUIRE EAGER EXECUTION ##\n",
        "\n",
        "        predicted_bsm = ae(bsm_data, training=False)\n",
        "        #mse_bsm = custom_loss_numpy(bsm_data, predicted_bsm.numpy())\n",
        "        mse_bsm = custom_loss_training(bsm_data, predicted_bsm)\n",
        "\n",
        "        #mse_true_val = np.concatenate((np.ones(bsm_data.shape[0]), np.zeros(X_test.shape[0])), axis=-1)\n",
        "        mse_true_val = tf.concat([tf.ones(bsm_data.shape[0]), tf.zeros(X_test.shape[0])], axis=-1)\n",
        "        #mse_pred_val = np.concatenate((mse_bsm, mse_qcd), axis=-1)\n",
        "        mse_pred_val=tf.concat([mse_bsm, mse_qcd], axis=-1)\n",
        "        #mse_fpr_loss, mse_tpr_loss, mse_threshold_loss = roc_curve(mse_true_val, mse_pred_val)\n",
        "        mse_fpr_loss, mse_tpr_loss, mse_threshold_loss = roc_curve(mse_true_val.numpy(), mse_pred_val.numpy())\n",
        "        \n",
        "        mse_objective = np.interp(10**(-5), mse_fpr_loss, mse_tpr_loss)\n",
        "        \n",
        "    \n",
        "        # WITH TF OPERATIONS (NO EAGER MODE)\n",
        "        #m = tf.keras.metrics.SensitivityAtSpecificity(specificity=1-(10**(-5)))\n",
        "        #mse_pred_val_np_norm = mse_pred_val_np / mse_pred_val_np.max()\n",
        "        #m.update_state(mse_true_val_np, mse_pred_val_np_norm)\n",
        "        #mse_objective_tf = m.result().numpy()\n",
        "        ## end TF operations ##        \n",
        "\n",
        "        objective = mse_objective # maximize\n",
        "        return objective\n",
        "    return roc_objective_val\n",
        "\n",
        "def load_model(model_name, custom_objects={'QDense': QDense, 'QActivation': QActivation}):\n",
        "    name = model_name + '.json'\n",
        "    json_file = open(name, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    model = model_from_json(loaded_model_json, custom_objects=custom_objects)\n",
        "    model.load_weights(model_name + '.h5')\n",
        "    return model\n",
        "\n",
        "def save_model(model_save_name, model):\n",
        "    with open(model_save_name + '.json', 'w') as json_file:\n",
        "        json_file.write(model.to_json())\n",
        "    model.save_weights(model_save_name + '.h5')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEWxvxbJxLNb"
      },
      "source": [
        "# Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba7e916"
      },
      "source": [
        "# TODO: Please set GCP_PROJECT_ID to your own Google Cloud project ID.\n",
        "GCP_PROJECT_ID = 'gm-cern-304701' #@param {type:\"string\"}\n",
        "\n",
        "# TODO: Change the Service Account Name to your own Service Account\n",
        "SERVICE_ACCOUNT_NAME = 'viziersa' #@param {type:\"string\"}\n",
        "SERVICE_ACCOUNT = f'{SERVICE_ACCOUNT_NAME}@{GCP_PROJECT_ID}.iam.gserviceaccount.com'\n",
        "\n",
        "# TODO: set GCS_BUCKET to your own Google Cloud Storage (GCS) bucket.\n",
        "GCS_BUCKET = 'gm-cern-qkeras-vizier' #@param {type:\"string\"}\n",
        "\n",
        "# DO NOT CHANGE: Currently only the 'us-central1' region is supported.\n",
        "REGION = 'us-central1'\n",
        "\n",
        "# TODO: Make sure that the service account can read the input files"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4466564a"
      },
      "source": [
        "# Set Tuning Specific parameters\n",
        "\n",
        "# OPTIONAL: You can change the job name to any string.\n",
        "JOB_NAME = 'qkeras-vizier' #@param {type:\"string\"}\n",
        "\n",
        "# OPTIONAL:  Set Number of concurrent tuning jobs that you would like to run.\n",
        "NUM_JOBS = 2 #@param {type:\"integer\"}\n",
        "\n",
        "# TODO: Set the study ID for this run. Study_ID can be any unique string.\n",
        "# Reusing the same Study_ID will cause the Tuner to continue tuning the\n",
        "# Same Study parameters. This can be used to continue on a terminated job,\n",
        "# or load stats from a previous study.\n",
        "STUDY_NUMBER = '00001' #@param {type:\"string\"}\n",
        "STUDY_ID = f'{GCP_PROJECT_ID}_{JOB_NAME}_{STUDY_NUMBER}'\n",
        "\n",
        "# Setting location were training logs and checkpoints will be stored\n",
        "GCS_BASE_PATH = f'gs://{GCS_BUCKET}/{JOB_NAME}/{STUDY_ID}'\n",
        "TENSORBOARD_LOGS_DIR = os.path.join(GCS_BASE_PATH,\"logs\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JA7lN7Ykgo"
      },
      "source": [
        "## Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYPUXFHdJaSP"
      },
      "source": [
        "# Using tfc.remote() to ensure this code only runs in notebook\n",
        "if not tfc.remote():\n",
        "\n",
        "    # Authentication for Kaggle Notebooks\n",
        "    if \"kaggle_secrets\" in sys.modules:\n",
        "        from kaggle_secrets import UserSecretsClient\n",
        "        UserSecretsClient().set_gcloud_credentials(project=GCP_PROJECT_ID)\n",
        "\n",
        "    # Authentication for Colab Notebooks\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        from google.colab import auth\n",
        "        auth.authenticate_user()\n",
        "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e963da1"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b39bf78"
      },
      "source": [
        "file=f'gs://{GCS_BUCKET}/Delphes_dataset_HALF.h5'\n",
        "#file = h5py.File('Delphes_dataset_HALF.h5', 'r')\n",
        "with tf.io.gfile.GFile(file, mode='rb') as input_file:\n",
        "    hfile = h5py.File(input_file, 'r')\n",
        "    #X_train_flatten = np.array(file['X_train_flatten'])\n",
        "    X_test_flatten = np.array(hfile['X_test_flatten'])\n",
        "    #X_val_flatten = np.array(file['X_val_flatten'])\n",
        "    #X_train_scaled = np.array(file['X_train_scaled'])\n",
        "    #X_test_scaled = np.array(file['X_test_scaled'])\n",
        "    #X_val_scaled = np.array(file['X_val_scaled'])\n",
        "    hfile.close()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23cb15bd"
      },
      "source": [
        "#file='Delphes_dataset_HALF.h5'\n",
        "BATCH_SIZE = 1024 \n",
        "AUTOTUNE=tf.data.AUTOTUNE\n",
        "EPOCHS = 25\n",
        "NUM_EVALS=25\n",
        "#NUM_TRAIN_EXAMPLES=trainds.cardinality().numpy()\n",
        "NUM_SAMPLES=3000000\n",
        "#STEPS_PER_EPOCH=NUM_SAMPLES//BATCH_SIZE\n",
        "STEPS_PER_EPOCH=NUM_SAMPLES//(BATCH_SIZE*NUM_EVALS)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58feee11"
      },
      "source": [
        "X_train_flatten_ds=tfio.IODataset.from_hdf5(file, '/X_train_flatten')\n",
        "#X_test_flatten_ds=tfio.IODataset.from_hdf5(file,'/X_test_flatten')\n",
        "X_val_flatten_ds=tfio.IODataset.from_hdf5(file, '/X_val_flatten')\n",
        "\n",
        "X_train_scaled_ds=tfio.IODataset.from_hdf5(file, '/X_train_scaled')\n",
        "X_val_scaled_ds=tfio.IODataset.from_hdf5(file,'/X_val_scaled')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5651aba"
      },
      "source": [
        "#X_train_flatten_ds=tf.data.Dataset.from_tensor_slices(X_train_flatten).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
        "#X_test_flatten_ds=tf.data.Dataset.from_tensor_slices(X_test_flatten)\n",
        "#X_val_flatten_ds=tf.data.Dataset.from_tensor_slices(X_val_flatten)\n",
        "\n",
        "#X_train_scaled_ds=tf.data.Dataset.from_tensor_slices(X_train_scaled).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
        "#X_val_scaled_ds=tf.data.Dataset.from_tensor_slices(X_val_scaled)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12fa7a1f"
      },
      "source": [
        "trainds=tf.data.Dataset.zip((X_train_flatten_ds, X_train_scaled_ds)).take(NUM_SAMPLES).cache()\n",
        "trainds=trainds.shuffle(NUM_SAMPLES).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
        "#trainds=trainds.shuffle(BATCH_SIZE*10)\n",
        "#trainds=trainds.batch(BATCH_SIZE, drop_remainder=True)\n",
        "#trainds=trainds.prefetch(AUTOTUNE)\n",
        "valds=tf.data.Dataset.zip((X_val_flatten_ds, X_val_scaled_ds)).take(NUM_SAMPLES).cache()\n",
        "valds=valds.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
        "#valds=valds.shuffle(BATCH_SIZE*10).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n",
        "#X_test_flatten_ds=X_test_flatten_ds.shuffle(BATCH_SIZE*10).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a27ae13"
      },
      "source": [
        "#list(trainds.as_numpy_iterator())[:1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ca94c5"
      },
      "source": [
        "## Load signal data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c25f44b",
        "outputId": "4ba88f27-bee3-4027-fc7b-3cce8f65aede"
      },
      "source": [
        "ato4file=f'gs://{GCS_BUCKET}/Ato4l_lepFilter_13TeV.h5'\n",
        "with tf.io.gfile.GFile(ato4file, mode='rb') as input_file:\n",
        "    ato4l = h5py.File(input_file, 'r')\n",
        "    ato4l = ato4l['Particles'][:]\n",
        "    ato4l = ato4l[:,:,:-1]\n",
        "\n",
        "import joblib\n",
        "datfile=f'gs://{GCS_BUCKET}/pt_scaled_VAE_new.dat'\n",
        "with tf.io.gfile.GFile(datfile, mode='rb') as input_file:\n",
        "    pT_scaler = joblib.load(input_file)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator StandardScaler from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7378f9f2"
      },
      "source": [
        "test_scaled_ato4l, test_notscaled_ato4l = preprocess_anomaly_data(pT_scaler, ato4l)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ccf525"
      },
      "source": [
        "### Set objective and  compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b167e2a3"
      },
      "source": [
        "bsm_data = test_notscaled_ato4l #input - data without any preprocessing\n",
        "#obj = roc_objective(autoencoder, X_test_flatten[:1000], bsm_data)\n",
        "#with strategy.scope():\n",
        "#    autoencoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss_training, run_eagerly=True) # just to make sure it runs in eager\n",
        "#autoencoder.summary()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4882586c"
      },
      "source": [
        "### Set AutoQKeras parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0825c73f"
      },
      "source": [
        "from qkeras import *\n",
        "from qkeras.utils import model_quantize\n",
        "from qkeras.qtools import run_qtools\n",
        "from qkeras.qtools import settings as qtools_settings\n",
        "import pprint"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7d7b822"
      },
      "source": [
        "## Define hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "25df7969",
        "outputId": "b8748745-f7ea-4854-c0cf-2608863bece8"
      },
      "source": [
        "HPS = kerastuner.engine.hyperparameters.HyperParameters()\n",
        "HPS.Choice('kernel_quantizer', [\"quantized_bits(2,1,1,alpha=1.0)\",\n",
        "                \"quantized_bits(4,2,1,alpha=1.0)\",\n",
        "                \"quantized_bits(6,2,1,alpha=1.0)\",\n",
        "                \"quantized_bits(8,3,1,alpha=1.0)\",\n",
        "                \"quantized_bits(10,3,1,alpha=1.0)\",\n",
        "                \"quantized_bits(12,4,1,alpha=1.0)\",\n",
        "                \"quantized_bits(14,4,1,alpha=1.0)\",\n",
        "                \"quantized_bits(16,6,1,alpha=1.0)\"   \n",
        "])\n",
        "HPS.Choice(\"bias_quantizer\", [\"quantized_bits(2,1,1)\",\n",
        "                \"quantized_bits(4,2,1)\",\n",
        "                \"quantized_bits(6,2,1)\",\n",
        "                \"quantized_bits(8,3,1)\"\n",
        "])\n",
        "HPS.Choice(\"q_activation\",[\"quantized_relu(2,1)\",\n",
        "                \"quantized_relu(3,1)\",\n",
        "                \"quantized_relu(4,2)\",\n",
        "                \"quantized_relu(6,2)\",\n",
        "                \"quantized_relu(8,3)\",\n",
        "                \"quantized_relu(10,3)\",\n",
        "                \"quantized_relu(12,4)\",\n",
        "                \"quantized_relu(14,4)\",\n",
        "                \"quantized_relu(16,6)\"\n",
        "])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'quantized_relu(2,1)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e128565"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ed731a"
      },
      "source": [
        "def build_model(hp):\n",
        "    latent_dim = 3\n",
        "    input_shape = 56\n",
        "    #strategy=tf.distribute.MirroredStrategy()\n",
        "\n",
        "    #with strategy.scope():\n",
        "    #encoder\n",
        "    inputArray = Input(shape=(input_shape,))\n",
        "    x = Activation('linear', name='block_1_act')(inputArray)\n",
        "     #   else QActivation(f'quantized_bits(16,6,1)')(inputArray)\n",
        "    x = BatchNormalization(name='bn_1')(x)\n",
        "    x = QDense(32, \n",
        "               kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "               use_bias=False, name='block_2_dense')(x)\n",
        "    x = BatchNormalization(name='bn_2')(x)\n",
        "    x = QActivation(hp.get('q_activation'), name='block_2_act')(x)\n",
        "    x = QDense(16, \n",
        "               kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "               use_bias=False, name='block_3_dense')(x)\n",
        "    x = BatchNormalization(name='bn_3')(x)\n",
        "    x = QActivation(hp.get('q_activation'), name='block_3_act')(x)\n",
        "    encoder = QDense(latent_dim, \n",
        "                     kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "                     name='output_encoder')(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "\n",
        "    #decoder\n",
        "    x = QDense(16, \n",
        "               kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "               use_bias=False, name='block_4_dense')(encoder)\n",
        "    x = BatchNormalization(name='bn_4')(x)\n",
        "    x = QActivation(hp.get('q_activation'), name='block_4_act')(x)\n",
        "    x = QDense(32, \n",
        "               kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "               use_bias=False, name='block_5_dense')(x)\n",
        "    x = BatchNormalization(name='bn_5')(x)\n",
        "    x = QActivation(hp.get('q_activation'), name='block_5_act')(x)\n",
        "    x = QDense(input_shape, \n",
        "               kernel_quantizer=hp.get('kernel_quantizer'),\n",
        "               name='output_dense')(x)\n",
        "    decoder = Activation('linear', name='output_act')(x)\n",
        "\n",
        "    #create autoencoder\n",
        "    autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
        "    autoencoder.compile(optimizer=keras.optimizers.Adam(),\n",
        "                        loss=custom_loss_training, \n",
        "                        #metrics=['val_loss']\n",
        "                        #run_eagerly=True\n",
        "                       ) # just to make sure it runs in eager\n",
        "\n",
        "    autoencoder.summary()\n",
        "    return autoencoder"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quWR1Tel0ngW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc78Hw4QP8UF"
      },
      "source": [
        "# Define callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1bb5e05"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, TensorBoard\n",
        "#outputdir='output_tfc'\n",
        "callbacks=[]\n",
        "#if pruning=='pruned':\n",
        " #   callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
        "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
        "#callbacks.append(TerminateOnNaN())\n",
        "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_tfc'.format(outputdir),monitor=\"val_loss\",verbose=1,save_best_only=True))\n",
        "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True))\n",
        "#callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=8, restore_best_weights=True))\n",
        "callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_LOGS_DIR))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkipD8Lf0o-j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkz2HxwLQEH2"
      },
      "source": [
        "# Set up Cloud Tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19e154d3",
        "outputId": "67cb8b1f-7199-4de7-8d86-c16e3c9ee4dc"
      },
      "source": [
        "from tensorflow_cloud import CloudTuner\n",
        "\n",
        "#distribution_strategy = None\n",
        "#if not tfc.remote():\n",
        "    # Using MirroredStrategy to use a single instance with multiple GPUs\n",
        "    # during remote execution while using no strategy for local.\n",
        "#    distribution_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "tuner = CloudTuner(\n",
        "    build_model,\n",
        "    project_id=GCP_PROJECT_ID,\n",
        "    project_name= JOB_NAME,\n",
        "    region=REGION,\n",
        "    objective='val_loss',\n",
        "    hyperparameters=HPS,\n",
        "    max_trials=10,\n",
        "    directory=GCS_BASE_PATH,\n",
        "    study_id=STUDY_ID,\n",
        "    overwrite=True,\n",
        "    #distribution_strategy=distribution_strategy\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:\n",
            "This application reports technical and operational details of your usage of\n",
            "Cloud Services in accordance with Google privacy policy, for more information\n",
            "please refer to https://policies.google.com/privacy. If you wish\n",
            "to opt-out, you may do so by running\n",
            "tensorflow_cloud.utils.google_api_client.optout_metrics_reporting().\n",
            "\n",
            "INFO:absl:Detected running in COLAB environment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "This application reports technical and operational details of your usage of\n",
            "Cloud Services in accordance with Google privacy policy, for more information\n",
            "please refer to https://policies.google.com/privacy. If you wish\n",
            "to opt-out, you may do so by running\n",
            "tensorflow_cloud.utils.google_api_client.optout_metrics_reporting().\n",
            "\n",
            "INFO:tensorflow:Study already exists: projects/gm-cern-304701/locations/us-central1/studies/gm-cern-304701_qkeras-vizier_00001.\n",
            "Load existing study...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Study already exists: projects/gm-cern-304701/locations/us-central1/studies/gm-cern-304701_qkeras-vizier_00001.\n",
            "Load existing study...\n",
            "INFO:absl:Detected running in COLAB environment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 56)]              0         \n",
            "_________________________________________________________________\n",
            "block_1_act (Activation)     (None, 56)                0         \n",
            "_________________________________________________________________\n",
            "bn_1 (BatchNormalization)    (None, 56)                224       \n",
            "_________________________________________________________________\n",
            "block_2_dense (QDense)       (None, 32)                1792      \n",
            "_________________________________________________________________\n",
            "bn_2 (BatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "block_2_act (QActivation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "block_3_dense (QDense)       (None, 16)                512       \n",
            "_________________________________________________________________\n",
            "bn_3 (BatchNormalization)    (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "block_3_act (QActivation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "output_encoder (QDense)      (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "block_4_dense (QDense)       (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "bn_4 (BatchNormalization)    (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "block_4_act (QActivation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "block_5_dense (QDense)       (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "bn_5 (BatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "block_5_act (QActivation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "output_dense (QDense)        (None, 56)                1848      \n",
            "_________________________________________________________________\n",
            "output_act (Activation)      (None, 56)                0         \n",
            "=================================================================\n",
            "Total params: 5,371\n",
            "Trainable params: 5,067\n",
            "Non-trainable params: 304\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5L7pWQ-0uZ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76ISGmTOQaBo"
      },
      "source": [
        "# Run hp tuning job"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8120b99"
      },
      "source": [
        "\n",
        "# Setting to run tuning remotely, you can run tuner locally to validate it works first.\n",
        "if tfc.remote():\n",
        "    tuner.search(x=trainds, validation_data=valds, \n",
        "                 epochs=NUM_EVALS, steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                 callbacks=callbacks)\n",
        "## Uncomment to test locally for 2 epochs\n",
        "#else:\n",
        "#     tuner.search(x=trainds, validation_data=valds,\n",
        "#                  epochs=1, # run locally for 1 epoch to test everything works\n",
        "#                  steps_per_epoch=STEPS_PER_EPOCH, \n",
        "#                  callbacks=callbacks)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dd6294"
      },
      "source": [
        "# If you are using a custom image you can install modules via requirements txt file.\n",
        "with open('requirements.txt','w') as f:\n",
        "    #f.write('pandas==1.1.5\\n')\n",
        "    #f.write('numpy==1.18.5\\n')\n",
        "    #f.write('tensorflow-cloud\\n')\n",
        "    #f.write('keras-tuner\\n')\n",
        "    #f.write('git+https://github.com/google/qkeras.git@master\\n')\n",
        "    f.write('qkeras\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d745a4f"
      },
      "source": [
        "# Optional: Some recommended base images. If you provide none the system will choose one for you.\n",
        "#TF_GPU_IMAGE= \"tensorflow/tensorflow:latest-devel-gpu\" #devel images have git\n",
        "#TF_CPU_IMAGE= \"tensorflow/tensorflow:latest-devel\"\n",
        "#TF_GPU_IMAGE= \"tensorflow/tensorflow:2.6.0rc0-gpu\"\n",
        "#TF_CPU_IMAGE= \"tensorflow/tensorflow:2.6.0rc0\"\n",
        "TF_GPU_IMAGE= \"gcr.io/deeplearning-platform-release/tf2-gpu.2-5\" #devel images have git\n",
        "TF_CPU_IMAGE= \"gcr.io/deeplearning-platform-release/tf2-cpu.2-5\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XH1MyBcGjTwR",
        "outputId": "20464a8d-bbd4-4786-be84-d46a15a03eaf"
      },
      "source": [
        "callbacks"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.callbacks.ReduceLROnPlateau at 0x7f7d2f0f7150>,\n",
              " <tensorflow.python.keras.callbacks.TensorBoard at 0x7f7d2f0f76d0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "543826f2",
        "outputId": "ce227880-459f-4429-f271-0c675306cb6f"
      },
      "source": [
        "#def _called_from_notebook_FIX():\n",
        "#    return False\n",
        "\n",
        "#from unittest.mock import patch\n",
        "\n",
        "#with patch('tensorflow_cloud.core.run._called_from_notebook', new=_called_from_notebook_FIX):\n",
        "\n",
        "tfc.run_cloudtuner(\n",
        "    num_jobs=NUM_JOBS,\n",
        "    distribution_strategy='auto',\n",
        "    requirements_txt='requirements.txt',\n",
        "    docker_config=tfc.DockerConfig(\n",
        "        parent_image=TF_GPU_IMAGE,\n",
        "        image_build_bucket= GCS_BUCKET # this option will trigger google cloud build. \n",
        "        ),\n",
        "    chief_config=tfc.MachineConfig(cpu_cores=8, memory=30, \n",
        "                                   accelerator_type='auto', accelerator_count=1\n",
        "                                   ),\n",
        "    #tfc.COMMON_MACHINE_CONFIGS['K80_2X'],\n",
        "    job_labels={'job': JOB_NAME},\n",
        "    service_account=SERVICE_ACCOUNT \n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validating environment and input parameters.\n",
            "Validation was successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE2OvxVc00U4"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $TENSORBOARD_LOGS_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un88MAr4QlwF"
      },
      "source": [
        "# Retrieve results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "746c50fc",
        "outputId": "890ac8df-4400-49e8-b05d-bac423d19b75"
      },
      "source": [
        "if not tfc.remote():\n",
        "    tuner.results_summary(1)\n",
        "    best_model = tuner.get_best_models(1)[0]\n",
        "    best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
        "\n",
        "    # References to best trial assets\n",
        "    best_trial_id = tuner.oracle.get_best_trials(1)[0].trial_id\n",
        "    best_trial_dir = tuner.get_trial_dir(best_trial_id)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Detected running in COLAB environment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results in gs://gm-cern-qkeras-vizier/qkeras-vizier/gm-cern-304701_qkeras-vizier_00001/qkeras-vizier\n",
            "Showing 1 best trials\n",
            "Objective(name='val_loss', direction='min')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Detected running in COLAB environment.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Trial summary\n",
            "Hyperparameters:\n",
            "bias_quantizer: quantized_bits(6,2,1)\n",
            "kernel_quantizer: quantized_bits(12,4,1,alpha=1.0)\n",
            "q_activation: quantized_relu(10,3)\n",
            "Score: 0.13285528123378754\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 56)]              0         \n",
            "_________________________________________________________________\n",
            "block_1_act (Activation)     (None, 56)                0         \n",
            "_________________________________________________________________\n",
            "bn_1 (BatchNormalization)    (None, 56)                224       \n",
            "_________________________________________________________________\n",
            "block_2_dense (QDense)       (None, 32)                1792      \n",
            "_________________________________________________________________\n",
            "bn_2 (BatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "block_2_act (QActivation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "block_3_dense (QDense)       (None, 16)                512       \n",
            "_________________________________________________________________\n",
            "bn_3 (BatchNormalization)    (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "block_3_act (QActivation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "output_encoder (QDense)      (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "block_4_dense (QDense)       (None, 16)                48        \n",
            "_________________________________________________________________\n",
            "bn_4 (BatchNormalization)    (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "block_4_act (QActivation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "block_5_dense (QDense)       (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "bn_5 (BatchNormalization)    (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "block_5_act (QActivation)    (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "output_dense (QDense)        (None, 56)                1848      \n",
            "_________________________________________________________________\n",
            "output_act (Activation)      (None, 56)                0         \n",
            "=================================================================\n",
            "Total params: 5,371\n",
            "Trainable params: 5,067\n",
            "Non-trainable params: 304\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://gm-cern-qkeras-vizier/qkeras-vizier/gm-cern-304701_qkeras-vizier_00001/qkeras-vizier/trial_5/checkpoints/epoch_24/checkpoint",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b05634bacf42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbest_hyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \"\"\"\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# Method only exists in this class for the docstring override.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36mget_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m    254\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \"\"\"\n\u001b[1;32m    254\u001b[0m         \u001b[0mbest_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_trials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mhm_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_distribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             model.load_weights(\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_checkpoint_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2296\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2916\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2917\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2918\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2919\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2920\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msm_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2937\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2939\u001b[0;31m     \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2940\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2941\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://gm-cern-qkeras-vizier/qkeras-vizier/gm-cern-304701_qkeras-vizier_00001/qkeras-vizier/trial_5/checkpoints/epoch_24/checkpoint"
          ]
        }
      ]
    }
  ]
}