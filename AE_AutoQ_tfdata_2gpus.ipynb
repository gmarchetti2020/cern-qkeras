{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f318056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qkeras in /opt/conda/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in /opt/conda/lib/python3.7/site-packages (from qkeras) (4.61.1)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from qkeras) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from qkeras) (0.6.0)\n",
      "Requirement already satisfied: keras-tuner>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from qkeras) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from qkeras) (1.19.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from qkeras) (49.6.0.post20210108)\n",
      "Requirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.7/site-packages (from qkeras) (2.5)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from qkeras) (0.24.2)\n",
      "Requirement already satisfied: pyparser in /opt/conda/lib/python3.7/site-packages (from qkeras) (1.0)\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.7/site-packages (from keras-tuner>=1.0.1->qkeras) (2.5.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from keras-tuner>=1.0.1->qkeras) (7.24.1)\n",
      "Requirement already satisfied: kt-legacy in /opt/conda/lib/python3.7/site-packages (from keras-tuner>=1.0.1->qkeras) (1.0.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from keras-tuner>=1.0.1->qkeras) (2.25.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from keras-tuner>=1.0.1->qkeras) (20.9)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.1->qkeras) (5.0.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23.1->qkeras) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23.1->qkeras) (1.0.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (0.1.6)\n",
      "Requirement already satisfied: six~=1.10 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.2.1->qkeras) (1.15.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (5.0.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.18.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (2.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (4.8.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (3.0.19)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.1.2)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->keras-tuner>=1.0.1->qkeras) (0.7.5)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->keras-tuner>=1.0.1->qkeras) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->keras-tuner>=1.0.1->qkeras) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner>=1.0.1->qkeras) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->keras-tuner>=1.0.1->qkeras) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->keras-tuner>=1.0.1->qkeras) (2.4.7)\n",
      "Requirement already satisfied: parse==1.6.5 in /opt/conda/lib/python3.7/site-packages (from pyparser->qkeras) (1.6.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner>=1.0.1->qkeras) (2021.5.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.3.4)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.30.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (3.16.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (2.0.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.34.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/jupyter/.local/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.12.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (4.5.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.1->qkeras) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.1->qkeras) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/jupyter/.local/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.1->qkeras) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user qkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c85aa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model,model_from_json\n",
    "from tensorflow.keras.layers import Input, InputLayer, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from qkeras import QDense, QActivation\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "#import tensorflow_model_optimization as tfmot\n",
    "#tsk = tfmot.sparsity.keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "#from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training\n",
    "from functions_dist import preprocess_anomaly_data, custom_loss_negative, custom_loss_training, save_model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca03a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d193eccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "for d in physical_devices:\n",
    "  print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710dd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Delphes_dataset_HALF.h5', 'r')\n",
    "#X_train_flatten = np.array(file['X_train_flatten'])\n",
    "X_test_flatten = np.array(file['X_test_flatten'])\n",
    "#X_val_flatten = np.array(file['X_val_flatten'])\n",
    "\n",
    "#X_train_scaled = np.array(file['X_train_scaled'])\n",
    "#X_test_scaled = np.array(file['X_test_scaled'])\n",
    "#X_val_scaled = np.array(file['X_val_scaled'])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e8a1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fc3b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file='Delphes_dataset_HALF.h5'\n",
    "BATCH_SIZE = 1024 \n",
    "AUTOTUNE=tf.data.AUTOTUNE\n",
    "EPOCHS = 25\n",
    "NUM_EVALS=25\n",
    "#NUM_TRAIN_EXAMPLES=trainds.cardinality().numpy()\n",
    "NUM_SAMPLES=3000000\n",
    "STEPS_PER_EPOCH=NUM_SAMPLES//BATCH_SIZE\n",
    "#STEPS_PER_EPOCH=NUM_SAMPLES//(BATCH_SIZE*NUM_EVALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7220193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten_ds=tfio.IODataset.from_hdf5(file, '/X_train_flatten')\n",
    "#X_test_flatten_ds=tfio.IODataset.from_hdf5(file,'/X_test_flatten')\n",
    "X_val_flatten_ds=tfio.IODataset.from_hdf5(file, '/X_val_flatten')\n",
    "\n",
    "X_train_scaled_ds=tfio.IODataset.from_hdf5(file, '/X_train_scaled')\n",
    "X_val_scaled_ds=tfio.IODataset.from_hdf5(file,'/X_val_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ecbbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_flatten_ds=tf.data.Dataset.from_tensor_slices(X_train_flatten).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "#X_test_flatten_ds=tf.data.Dataset.from_tensor_slices(X_test_flatten)\n",
    "#X_val_flatten_ds=tf.data.Dataset.from_tensor_slices(X_val_flatten)\n",
    "\n",
    "#X_train_scaled_ds=tf.data.Dataset.from_tensor_slices(X_train_scaled).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "#X_val_scaled_ds=tf.data.Dataset.from_tensor_slices(X_val_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be599b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds=tf.data.Dataset.zip((X_train_flatten_ds, X_train_scaled_ds)).take(NUM_SAMPLES)\n",
    "trainds=trainds.shuffle(10*BATCH_SIZE).repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "#trainds=trainds.shuffle(BATCH_SIZE*10)\n",
    "#trainds=trainds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "#trainds=trainds.prefetch(AUTOTUNE)\n",
    "valds=tf.data.Dataset.zip((X_val_flatten_ds, X_val_scaled_ds)).take(NUM_SAMPLES)\n",
    "valds=valds.repeat(EPOCHS).batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "#valds=valds.shuffle(BATCH_SIZE*10).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n",
    "#X_test_flatten_ds=X_test_flatten_ds.shuffle(BATCH_SIZE*10).batch(BATCH_SIZE,drop_remainder=True).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8c6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(trainds.as_numpy_iterator())[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea3322d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 56)]              0         \n",
      "_________________________________________________________________\n",
      "block_1_act (Activation)     (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 56)                224       \n",
      "_________________________________________________________________\n",
      "block_2_dense (Dense)        (None, 32)                1792      \n",
      "_________________________________________________________________\n",
      "bn_2 (BatchNormalization)    (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "block_2_act (Activation)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "block_3_dense (Dense)        (None, 16)                512       \n",
      "_________________________________________________________________\n",
      "bn_3 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "block_3_act (Activation)     (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "output_encoder (Dense)       (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "block_4_dense (Dense)        (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "bn_4 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "block_4_act (Activation)     (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "block_5_dense (Dense)        (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "bn_5 (BatchNormalization)    (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "block_5_act (Activation)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "output_dense (Dense)         (None, 56)                1848      \n",
      "_________________________________________________________________\n",
      "output_act (Activation)      (None, 56)                0         \n",
      "=================================================================\n",
      "Total params: 5,371\n",
      "Trainable params: 5,067\n",
      "Non-trainable params: 304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 56\n",
    "#strategy=tf.distribute.MirroredStrategy()\n",
    "\n",
    "#with strategy.scope():\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape,))\n",
    "x = Activation('linear', name='block_1_act')(inputArray)\n",
    " #   else QActivation(f'quantized_bits(16,6,1)')(inputArray)\n",
    "x = BatchNormalization(name='bn_1')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_2_dense')(x)\n",
    "x = BatchNormalization(name='bn_2')(x)\n",
    "x = Activation('relu', name='block_2_act')(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_3_dense')(x)\n",
    "x = BatchNormalization(name='bn_3')(x)\n",
    "x = Activation('relu', name='block_3_act')(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),name='output_encoder')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_4_dense')(encoder)\n",
    "x = BatchNormalization(name='bn_4')(x)\n",
    "x = Activation('relu', name='block_4_act')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_5_dense')(x)\n",
    "x = BatchNormalization(name='bn_5')(x)\n",
    "x = Activation('relu', name='block_5_act')(x)\n",
    "x = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(), name='output_dense')(x)\n",
    "decoder = Activation('linear', name='output_act')(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss_training, \n",
    "                    #run_eagerly=True\n",
    "                   ) # just to make sure it runs in eager\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0751a",
   "metadata": {},
   "source": [
    "## Train without quantization\n",
    "This is just to understnd the timings involved and test the process so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7195c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "odir='output'\n",
    "callbacks=[]\n",
    "#if pruning=='pruned':\n",
    " #   callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "#callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best.h5'.format(odir),monitor=\"val_loss\",verbose=1,save_best_only=True))\n",
    "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True))\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c756b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "2929/2929 [==============================] - 114s 38ms/step - loss: 0.1990 - val_loss: 0.3871\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38710, saving model to output/AUTOQKERAS_best.h5\n",
      "Epoch 2/25\n",
      "2929/2929 [==============================] - 99s 34ms/step - loss: 0.1464 - val_loss: 0.3807\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38710 to 0.38073, saving model to output/AUTOQKERAS_best.h5\n",
      "Epoch 3/25\n",
      "2929/2929 [==============================] - 99s 34ms/step - loss: 0.1321 - val_loss: 0.7396\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.38073\n",
      "Epoch 4/25\n",
      "2924/2929 [============================>.] - ETA: 0s - loss: 0.1244"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b429be135a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m            \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                        verbose=1)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1223\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1226\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "history=autoencoder.fit(x=trainds, \n",
    "           validation_data=valds, \n",
    "           #batch_size=BATCH_SIZE,\n",
    "           epochs=EPOCHS,\n",
    "           #epochs=NUM_EVALS,\n",
    "           steps_per_epoch=STEPS_PER_EPOCH,\n",
    "           callbacks=callbacks,\n",
    "                       verbose=1)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16357534",
   "metadata": {},
   "source": [
    "### Load signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ato4l = h5py.File('Ato4l_lepFilter_13TeV.h5', 'r')\n",
    "ato4l = ato4l['Particles'][:]\n",
    "ato4l = ato4l[:,:,:-1]\n",
    "\n",
    "import joblib\n",
    "pT_scaler = joblib.load('pt_scaled_VAE_new.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ato4l, test_notscaled_ato4l = preprocess_anomaly_data(pT_scaler, ato4l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6396b420",
   "metadata": {},
   "source": [
    "### Set objective and  compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09d7d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_data = test_notscaled_ato4l #input - data without any preprocessing\n",
    "#obj = roc_objective(autoencoder, X_test_flatten[:1000], bsm_data)\n",
    "#with strategy.scope():\n",
    "#    autoencoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss_training, run_eagerly=True) # just to make sure it runs in eager\n",
    "#autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fc8752",
   "metadata": {},
   "source": [
    "### Override AutoQKeras classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_AutoQKeras_dist import *\n",
    "#from Custom_AutoQKeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25ab2a",
   "metadata": {},
   "source": [
    "### Set AutoQKeras parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_internal = \"fp32\"\n",
    "reference_accumulator = \"fp32\"\n",
    "\n",
    "q = run_qtools.QTools(\n",
    "  autoencoder,\n",
    "  # energy calculation using a given process\n",
    "  # \"horowitz\" refers to 45nm process published at\n",
    "  # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n",
    "  # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n",
    "  # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14, \n",
    "  # doi: 10.1109/ISSCC.2014.6757323.\n",
    "  process=\"horowitz\",\n",
    "  # quantizers for model input\n",
    "  source_quantizers=[quantized_bits(16, 6, 1)],\n",
    "  is_inference=False,\n",
    "  # absolute path (including filename) of the model weights\n",
    "  # in the future, we will attempt to optimize the power model\n",
    "  # by using weight information, although it can be used to further\n",
    "  # optimize QBatchNormalization.\n",
    "  weights_path=None,\n",
    "  # keras_quantizer to quantize weight/bias in un-quantized keras layers\n",
    "  keras_quantizer=reference_internal,\n",
    "  # keras_quantizer to quantize MAC in un-quantized keras layers\n",
    "  keras_accumulator=reference_accumulator,\n",
    "  # whether calculate baseline energy\n",
    "  for_reference=True)\n",
    "  \n",
    "# caculate energy of the derived data type map.\n",
    "energy_dict = q.pe(\n",
    "    # whether to store parameters in dram, sram, or fixed\n",
    "    weights_on_memory=\"sram\",\n",
    "    # store activations in dram or sram\n",
    "    activations_on_memory=\"sram\",\n",
    "    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "    min_sram_size=8*16*1024*1024,\n",
    "    rd_wr_on_io=False)\n",
    "\n",
    "# get stats of energy distribution in each layer\n",
    "energy_profile = q.extract_energy_profile(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "# extract sum of energy of each layer according to the rule specified in\n",
    "# qtools_settings.cfg.include_energy\n",
    "total_energy = q.extract_energy_sum(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "\n",
    "pprint.pprint(energy_profile)\n",
    "print()\n",
    "print(\"Total energy: {:.2f} uJ\".format(total_energy / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(4,2,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(6,2,1,alpha=1.0)\": 6,\n",
    "                \"quantized_bits(8,3,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,3,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,4,1,alpha=1.0)\": 12,\n",
    "                \"quantized_bits(14,4,1,alpha=1.0)\": 14,\n",
    "                \"quantized_bits(16,6,1,alpha=1.0)\": 16\n",
    "        },\n",
    "        \"bias\": {\n",
    "                \"quantized_bits(2,1,1)\": 2,\n",
    "                \"quantized_bits(4,2,1)\": 4,\n",
    "                \"quantized_bits(6,2,1)\": 6,\n",
    "                \"quantized_bits(8,3,1)\": 8\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_relu(2,1)\": 2,\n",
    "                \"quantized_relu(3,1)\": 3,\n",
    "                \"quantized_relu(4,2)\": 4,\n",
    "                \"quantized_relu(6,2)\": 6,\n",
    "                \"quantized_relu(8,3)\": 8,\n",
    "                \"quantized_relu(10,3)\": 10,\n",
    "                \"quantized_relu(12,4)\": 12,\n",
    "                \"quantized_relu(14,4)\": 14,\n",
    "                \"quantized_relu(16,6)\": 16\n",
    "        },\n",
    "        \"linear\": {\n",
    "                \"quantized_bits(16,6)\": 16\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05899c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [16, 8, 16],\n",
    "    \"Activation\": [16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d6981",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 4.0, # a try\n",
    "        \"stress\": 0.6, # a try\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"fp16\"],\n",
    "        \"reference_internal\": \"fp16\",\n",
    "        \"reference_accumulator\": \"fp16\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1278c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "odir='autoqkeras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"output_dir\": \"{}/\".format(odir),\n",
    "    \"goal\": goal,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"learning_rate_optimizer\": False,\n",
    "    \"transfer_weights\": False,\n",
    "    \"mode\": \"bayesian\", \n",
    "    #\"max_epochs\": 25, #changed for hyperband\n",
    "    \"score_metric\": \"val_custom_score\", # must have a metric for keras tuner to save\n",
    "    \"seed\": 42,\n",
    "    \"limit\": limit,\n",
    "    \"tune_filters\": \"layer\",\n",
    "    \"tune_filters_exceptions\": \"^output.*\",\n",
    "    \"layer_indexes\": [1,3,5,6,8,9,10,12,13,15,16,17],\n",
    "    \"max_trials\": 130,\n",
    "    #\"distribution_strategy\":strategy, #changed\n",
    "    \"blocks\": [\n",
    "          \"block_1_.*$\",\n",
    "          \"block_2_.*$\",\n",
    "          \"block_3_.*$\",\n",
    "          \"output_encoder$\",\n",
    "          \"block_4_.*$\",\n",
    "          \"block_5_.*$\",\n",
    "          \"output_dense$\",\n",
    "          \"output_act$\",],\n",
    "    \"schedule_block\": \"cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"quantizing layers:\", [autoencoder.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfa416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "outputdir='output'\n",
    "callbacks=[]\n",
    "#if pruning=='pruned':\n",
    " #   callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "#callbacks.append(TerminateOnNaN())\n",
    "callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best'.format(outputdir),monitor=\"val_loss\",verbose=1,save_best_only=True))\n",
    "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True))\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16386f",
   "metadata": {},
   "source": [
    "### Run search with AutoQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549aba42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoqk = Custom_AutoQKerasScheduler(autoencoder,\n",
    "                                    metrics=[custom_loss_negative],\n",
    "                                    X_test = X_test_flatten[:NUM_SAMPLES],\n",
    "                                    bsm_data = bsm_data,\n",
    "                                    custom_objects={},\n",
    "                                    debug=False,\n",
    "                                    **run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260121eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "autoqk.fit(x=trainds, \n",
    "           validation_data=valds, \n",
    "           #batch_size=BATCH_SIZE,\n",
    "           #epochs=EPOCHS,\n",
    "           epochs=NUM_EVALS,\n",
    "           steps_per_epoch=STEPS_PER_EPOCH,\n",
    "           callbacks=callbacks)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.summary()\n",
    "save_model('best_pretrain_objective_roc', qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=tf.keras.metrics.SensitivityAtSpecificity(1-1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.update_state([1,1,0], [2.0,0.4,0.8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353d208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
